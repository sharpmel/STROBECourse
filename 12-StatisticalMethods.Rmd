---
title: "12-StatisticalMethods"
author: "Melissa K Sharp"
delete_merged_file: true
site: bookdown::bookdown_site
output:
  bookdown::gitbook:
    number_sections: false 
    includes: 
      after_body: disqus.html
bibliography: bibliography.bib
csl: apa-annotated.csl
link-citations: yes
edit: https://github.com/sharpmel/STROBECourse
---

# Methods: Statistical Methods (12)
> The items from STROBE state that you should report:   
-	Describe all statistical methods, including those used to control for confounding (12a) 
-	Describe any methods used to examine subgroups and interactions (12b)    
-	Explain how missing data were addressed (12c)  
-	Cohort study If applicable, explain how loss to follow up was addressed (12d)  
-	Case-control study If applicable, explain how matching of cases and controls was addressed (12d)  
-	Cross-sectional study If applicable, describe analytical methods taking account of sampling strategy (12d)  
-	Describe any sensitivity analyses (12e)  

</br>  

**Some key items to consider adding:**  
- All statistical methods for each objective at a level of detail sufficient for a knowledgeable reader to replicate the methods  
- Clearly indicate the unit of analysis (e.g., individual, team, family, unit, etc.)  
- The validity and reliability of any measurements used 
- If any internal/external validation was done  
- How items/variables were selected/introduced into statistical models  
- Data analysis software version and options/settings used   
- If the same association under study has previously been published, consider using a similar analysis model and deﬁnitions for replicative purposes  
- **Methods used to**   
 -- Assess robustness of analyses (e.g, sensitivity analyses, quantitative bias assessment)
 -- Adjust for measurement error, (i.e., from a validity or calibration study)  
 -- Account for (complex) sampling strategy (e.g., estimator used)  
 -- Address missing data or loss-to-follow-up  
 -- Control for confounding  
 -- Manage and correct for for non-independence (i.e., relatedness) of data  
 -- Address multiple comparisons or to control for the risk of false positive findings  
 -- Assess and address population stratification  
 -- Identify and address repeated measures on subjects
 -- Clean data  
 -- Match, combine, or link data (person/individual/dataset level linkages) and an evaluation of the linkage quality  

</br>

## Explanation 12a  
> In general, there is no one correct statistical analysis but, rather, several possibilities that may address the same question, but make different assumptions. Regardless, investigators should pre-determine analyses at least for the primary study objectives in a study protocol. Often additional analyses are needed, either instead of, or as well as, those originally envisaged, and these may sometimes be motivated by the data. When a study is reported, authors should tell readers whether particular analyses were suggested by data inspection. Even though the distinction between pre-specified and exploratory analyses may sometimes be blurred, authors should clarify reasons for particular analyses.  
</br>
If groups being compared are not similar with regard to some characteristics, adjustment should be made for possible confounding variables by stratification or by multivariable regression (see [box 5][Box 5. Confounding]). [@slama2005] Often, the study design determines which type of regression analysis is chosen. For instance, Cox proportional hazard regression is commonly used in cohort studies, [@greenland1998] whereas logistic regression is often the method of choice in case-control studies. [@thompson1994; @schlesselman1982]. Analysts should fully describe specific procedures for variable selection and not only present results from the final model. [@clayton1993; @altman1983] If model comparisons are made to narrow down a list of potential confounders for inclusion in a final model, this process should be described. It is helpful to tell readers if one or two covariates are responsible for a great deal of the apparent confounding in a data analysis. Other statistical analyses such as imputation procedures, data transformation, and calculations of attributable risks should also be described. Nonstandard or novel approaches should be referenced and the statistical software used reported. As a guiding principle, we advise statistical methods be described “with enough detail to enable a knowledgeable reader with access to the original data to verify the reported results.” [@icmje1997]   
</br>
In an empirical study, only 93 of 169 articles (55%) reporting adjustment for confounding clearly stated how continuous and multi-category variables were entered into the statistical model. [@mullner2002] Another study found that among 67 articles in which statistical analyses were adjusted for confounders, it was mostly unclear how confounders were chosen. [@pocock2004]  

</br>

## Examples 12a
- “The adjusted relative risk was calculated using the Mantel-Haenszel technique, when evaluating if confounding by age or gender was present in the groups compared. The 95% confidence interval (CI) was computed around the adjusted relative risk, using the variance according to Greenland and Robins and Robins et al.” [@berglund2001; @vandenbroucke2007].  

</br></br>

## Explanation 12b  
> As discussed in detail under [item 17][Results: Other Analyses (17)], many debate the use and value of analyses restricted to subgroups of the study population. [@pocock2004; @gotzsche2006] Subgroup analyses are nevertheless often done. [@pocock2004] Readers need to know which subgroup analyses were planned in advance, and which arose while analyzing the data. Also, it is important to explain what methods were used to examine whether effects or associations differed across groups (see [item 17][Results: Other Analyses (17)]).  
</br>
Interaction relates to the situation when one factor modifies the effect of another (therefore also called ‘effect modification’). The joint action of two factors can be characterized in two ways: on an additive scale, in terms of risk differences; or on a multiplicative scale, in terms of relative risk (see [box 8][Box 8. Interaction (effect modification)]). Many authors and readers may have their own preference about the way interactions should be analyzed. Still, they may be interested to know to what extent the joint effect of exposures differs from the separate effects. There is consensus that the additive scale, which uses absolute risks, is more appropriate for public health and clinical decision making. [@szklo2000] Whatever view is taken, this should be clearly presented to the reader, as is done in the example above. [@hallan2006] A lay-out presenting separate effects of both exposures as well as their joint effect, each relative to no exposure, might be most informative. It is presented in the example for interaction under [item 17][Results: Other Analyses (17)], and the calculations on the different scales are explained in [box 8][Box 8. Interaction (effect modification)].  

</br>

## Examples 12b  
- “Sex differences in susceptibility to the 3 lifestyle-related risk factors studied were explored by testing for biological interaction according to Rothman: a new composite variable with 4 categories (a−b−, a−b+, a+b−, and a+b+) was redefined for sex and a dichotomous exposure of interest where a− and b− denote absence of exposure. RR was calculated for each category after adjustment for age. An interaction effect is defined as departure from additivity of absolute effects, and excess RR caused by interaction (RERI) was calculated:  
</br>
<img align="center" src="images/12b.png" alt="RERI">

</br>
- where RR(a+b+) denotes RR among those exposed to both factors where RR(a−b−) is used as reference category (RR = 1.0). Ninety-five percent CIs were calculated as proposed by Hosmer and Lemeshow. RERI of 0 means no interaction” [@hallan2006; @vandenbroucke2007].  

## Box 5. Confounding  
Confounding literally means confusion of effects. A study might seem to show either an association or no association between an exposure and the risk of a disease. In reality, the seeming association or lack of association is due to another factor that determines the occurrence of the disease but that is also associated with the exposure. The other factor is called the confounding factor or confounder. Confounding thus gives a wrong assessment of the potential ‘causal' association of an exposure. For example, if women who approach middle age and develop elevated blood pressure are less often prescribed oral contraceptives, a simple comparison of the frequency of cardiovascular disease between those who use contraceptives and those who do not, might give the wrong impression that contraceptives protect against heart disease.  
</br>
Investigators should think beforehand about potential confounding factors. This will inform the study design and allow proper data collection by identifying the confounders for which detailed information should be sought. Restriction or matching may be used. In the example above, the study might be restricted to women who do not have the confounder, elevated blood pressure. Matching on blood pressure might also be possible, though not necessarily desirable (see [Box 2][Box 2. Matching]). In the analysis phase, investigators may use stratification or multivariable analysis to reduce the effect of confounders. Stratification consists of dividing the data in strata for the confounder (e.g., strata of blood pressure), assessing estimates of association within each stratum, and calculating the combined estimate of association as a weighted average over all strata. Multivariable analysis achieves the same result but permits one to take more variables into account simultaneously. It is more flexible but may involve additional assumptions about the mathematical form of the relationship between exposure and disease.  
</br>
Taking confounders into account is crucial in observational studies, but readers should not assume that analyses adjusted for confounders establish the ‘causal part' of an association. Results may still be distorted by residual confounding (the confounding that remains after unsuccessful attempts to control for it [@olsen1999], random sampling error, selection bias and information bias (see [Box 3][Box 3. Bias]).  
</br>

## Explanation 12c  
> Missing data are common in observational research. Questionnaires posted to study participants are not always filled in completely, participants may not attend all follow-up visits and routine data sources and clinical databases are often incomplete. Despite its ubiquity and importance, few papers report in detail on the problem of missing data. [@tooth2005; @vach1991] Investigators may use any of several approaches to address missing data. We describe some strengths and limitations of various approaches in [box 6][Box 6. Missing data]. We advise that authors report the number of missing values for each variable of interest (exposures, outcomes, confounders) and for each step in the analysis. Authors should give reasons for missing values if possible, and indicate how many individuals were excluded because of missing data when describing the flow of participants through the study (see also [item 13][Results: Participants (13)]). For analyses that account for missing data, authors should describe the nature of the analysis (eg, multiple imputation) and the assumptions that were made (eg, missing at random, see [box 6][Box 6. Missing data]). [@vandenbroucke2007]  

</br>

## Examples 12c  
- “Our missing data analysis procedures used missing at random (MAR) assumptions. We used the MICE (multivariate imputation by chained equations) method of multiple multivariate imputation in STATA. We independently analysed 10 copies of the data, each with missing values suitably imputed, in the multivariate logistic regression analyses. We averaged estimates of the variables to give a single mean estimate and adjusted standard errors according to Rubin's rules” [@chandola2006; @vandenbroucke2007].

## Explanation 12d Cohort  
> Cohort studies are analyzed using life table methods or other approaches that are based on the person-time of follow- up and time to developing the disease of interest. Among individuals who remain free of the disease at the end of their observation period, the amount of follow-up time is assumed to be unrelated to the probability of developing the outcome. This will be the case if follow-up ends on a fixed date or at a particular age. Loss to follow-up occurs when participants withdraw from a study before that date. This may hamper the validity of a study if loss to follow-up occurs selectively in exposed individuals, or in persons at high risk of developing the disease ('informative censoring’). In the example above, patients lost to follow-up in treatment programs with no active follow-up had fewer CD4 helper cells than those remaining under observation and were therefore at higher risk of dying. [@braitstein2006]  
</br> 
It is important to distinguish persons who reach the end of the study from those lost to follow-up. Unfortunately, statistical software usually does not distinguish between the two situations: in both cases follow-up time is automatically truncated (‘censored’) at the end of the observation period. Investigators therefore need to decide, ideally at the stage of planning the study, how they will deal with loss to follow-up. When few patients are lost, investigators may either exclude individuals with incomplete follow-up, or treat them as if they withdrew alive at either the date of loss to follow-up or the end of the study. We advise authors to report how many patients were lost to follow-up and what censoring strategies they used.  

</br>

## Box 6. Missing data  
**Problems and possible solutions**  
A common approach to dealing with missing data is to restrict analyses to individuals with complete data on all variables required for a particular analysis. Although such ‘complete-case' analyses are unbiased in many circumstances, they can be biased and are always inefficient [@little2002]. Bias arises if individuals with missing data are not typical of the whole sample. Inefficiency arises because of the reduced sample size for analysis.  
</br>
Using the last observation carried forward for repeated measures can distort trends over time if persons who experience a foreshadowing of the outcome selectively drop out [@ware2003]. Inserting a missing category indicator for a confounder may increase residual confounding [@vach1991]. Imputation, in which each missing value is replaced with an assumed or estimated value, may lead to attenuation or exaggeration of the association of interest, and without the use of sophisticated methods described below may produce standard errors that are too small.  
</br>
Rubin developed a typology of missing data problems, based on a model for the probability of an observation being missing [@little2002; @rubin1976]. Data are described as missing completely at random (MCAR) if the probability that a particular observation is missing does not depend on the value of any observable variable(s). Data are missing at random (MAR) if, given the observed data, the probability that observations are missing is independent of the actual values of the missing data. For example, suppose younger children are more prone to missing spirometry measurements, but that the probability of missing is unrelated to the true unobserved lung function, after accounting for age. Then the missing lung function measurement would be MAR in models including age. Data are missing not at random (MNAR) if the probability of missing still depends on the missing value even after taking the available data into account. When data are MNAR valid inferences require explicit assumptions about the mechanisms that led to missing data.  
</br>
Methods to deal with data missing at random (MAR) fall into three broad classes [@little2002; @schafer1997]: likelihood-based approaches [@lipsitz1999], weighted estimation [@rotnitzky1997] and multiple imputation [@little2002; @rubin1987]. Of these three approaches, multiple imputation is the most commonly used and flexible, particularly when multiple variables have missing values [@barnard1999]. Results using any of these approaches should be compared with those from complete case analyses, and important differences discussed. The plausibility of assumptions made in missing data analyses is generally unverifiable. In particular it is impossible to prove that data are MAR, rather than MNAR. Such analyses are therefore best viewed in the spirit of sensitivity analysis (see items [12e][Explanation 12e] and [17][Results: Other Analyses (17)]). [@vandenbroucke2007]

## Explanation 12d Case-Control  
> In individually matched case-control studies a crude analysis of the odds ratio, ignoring the matching, usually leads to an estimation that is biased towards unity (see [box 2][Box 2. Matching]). A matched analysis is therefore often necessary. This can intuitively be understood as a stratified analysis: each case is seen as one stratum with his or her set of matched controls. The analysis rests on considering whether the case is more often exposed than the controls, despite having made them alike regarding the matching variables. Investigators can do such a stratified analysis using the Mantel-Haenszel method on a ‘matched’ 2 by 2 table. In its simplest form the odds ratio becomes the ratio of pairs that are discordant for the exposure variable. If matching was done for variables like age and sex that are universal attributes, the analysis needs not retain the individual, person-to-person matching: a simple analysis in categories of age and sex is sufficient. [@rothmanmatching1998] For other matching variables, such as neighborhood, sibship, or friendship, however, each matched set should be considered its own stratum. In individually matched studies, the most widely used method of analysis is conditional logistic regression, in which each case and their controls are considered together. The conditional method is necessary when the number of controls varies among cases, and when, in addition to the matching variables, other variables need to be adjusted for. To allow readers to judge whether the matched design was appropriately taken into account in the analysis, we recommend that authors describe in detail what statistical methods were used to analyse the data. If taking the matching into account does have little effect on the estimates, authors may choose to present an unmatched analysis. [@vandenbroucke2007] 
</br>

## Explanation 12d Cross-sectional  
> Most cross-sectional studies use a pre-specified sampling strategy to select participants from a source population. Sampling may be more complex than taking a simple random sample, however. It may include several stages and clustering of participants (eg, in districts or villages). Proportionate stratification may ensure that subgroups with a specific characteristic are correctly represented. Disproportionate stratification may be useful to over-sample a subgroup of particular interest.  
</br>
An estimate of association derived from a complex sample may be more or less precise than that derived from a simple random sample. Measures of precision such as standard error or confidence interval should be corrected using the design effect, a ratio measure that describes how much precision is gained or lost if a more complex sampling strategy is used instead of simple random sampling. [@lohr1999] Most complex sampling techniques lead to a decrease of precision, resulting in a design effect greater than 1.   
</br>
We advise that authors clearly state the method used to adjust for complex sampling strategies so that readers may understand how the chosen sampling method influenced the precision of the obtained estimates. For instance, with clustered sampling, the implicit trade-off between easier data collection and loss of precision is transparent if the design effect is reported. In the example, the calculated design effects of 1.9 for men indicates that the actual sample size would need to be 1.9 times greater than with simple random sampling for the resulting estimates to have equal precision. [@vandenbroucke2007]    

</br> 

## Explanation 12e  
> Sensitivity analyses are useful to investigate whether or not the main results are consistent with those obtained with alternative analysis strategies or assumptions. [@rothman1998sensitivity] Issues that may be examined include the criteria for inclusion in analyses, the definitions of exposures or outcomes, [@custer2006] which confounding variables merit adjustment, the handling of missing data, [@dunn2001; @wakefield2000] possible selection bias or bias from inaccurate or inconsistent measurement of exposure, disease and other variables, and specific analysis choices, such as the treatment of quantitative variables (see [item 11][Methods: Quantitative variables (11)]). Sophisticated methods are increasingly used to simultaneously model the influence of several biases or assumptions. [@greenland2003; @lash2003; @phillips2003]  
</br> 
In 1959 Cornfield et al famously showed that a relative risk of 9 for cigarette smoking and lung cancer was extremely unlikely to be due to any conceivable confounder, since the confounder would need to be at least nine times as prevalent in smokers as in non-smokers. [@cornfield1959] This analysis did not rule out the possibility that such a factor was present, but it did identify the prevalence such a factor would need to have. The same approach was recently used to identify plausible confounding factors that could explain the association between childhood leukemia and living near electric power lines. [@langholz2001] More generally, sensitivity analyses can be used to identify the degree of confounding, selection bias, or information bias required to distort an association. One important, perhaps under recognized, use of sensitivity analysis is when a study shows little or no association between an exposure and an outcome and it is plausible that confounding or other biases toward the null are present. [@vandenbroucke2007]  

</br></br>

## Field-specific guidance   
**Genetic association studies [@little2009]**  
-	State whether Hardy-Weinberg equilibrium was considered and, if so, how  
-	Describe any methods used for inferring genotypes or haplotypes  </br> 

**Nutritional data [@lachat2016]**  
-	Describe and justify the method for energy adjustments, intake modeling, and use of weighting factors, if applicable  </br>

**Response-driven sampling [@white2015]**  
-	Report any criteria used to support statements on whether estimator conditions or assumptions were appropriate  
-	Explain how seeds were handled in analysis  </br>

**Rheumatology[@zavada2014]**  
-	Deﬁne and justify the risk window. Whenever possible, categorise as (1) on drug, (2) on drug + lag window or (3) ever treated  
-	The use of multiple risk attribution models and lag windows is encouraged if appropriate, but needs to be accompanied by a description of numbers and relative risks for each model  </br>

**Routinely collected health data [@benchimol2015]**  
-	Authors should describe the extent to which the investigators had access to the database population used to create the study population  </br>

**Seroepidemiologic studies for influenza [@horby2017]**  
- If relevant, report methods used to account for the probability of seropositivity or seroconversion if infected, and to account for decay in antibody titers over time  
-	Describe the sample type—serum or plasma. If plasma is used, specify the anticoagulant used (heparin, sodium citrate, EDTA, etc.)  
-	Describe the specimen storage conditions (4°C, −20 °C, −80 °C). If frozen prior to the analysis, describe the time to freezing and the number of freeze/thaw cycles prior to testing  
-	Specify the assay type (e.g., hemagglutination inhibition; virus neutralization/microneutralization; ELISA; other) and methods used to determine the endpoint titer  
-	Reference a previously published, CONSISE consensus serologic assay or WHO protocol if used, and any modifications of the protocol. If a previously published protocol is not used, provide full details in supplementary materials  
- State what is known about the determinants of the variability of the antibody detection assay being used  
-	Specify the antigen(s) used in the assay, including virus strain name, subtype, lineage or clade, with standardized nomenclature and reference; specify whether live virus or inactivated virus was used (where applicable)  
-	Report if antigen(s) from potentially cross-reactive pathogens/strains were used in order to identify cross-reactivity, and specify which antigen was used, including virus name, subtype, strain, lineage and clade, with standardized nomenclature and reference  
-	If red blood cells were used for a hemagglutinin inhibition assay, specify the animal species from which they were obtained and concentration (v/v) used  
-	Describe positive and negative controls used  
-	Describe starting and end dilutions  
-	Specify laboratory biosafety conditions  
-	Specify whether replication was performed, and if so, the acceptable replication parameters  
-	Specify whether a confirmatory assay was performed and all specifics of this assay, at the same level of detail  
-	Specify international standards used, if appropriate  

</br>

## Resources   
Do you know of any good guidance or resources related to this item? Suggest them via comments below, [Twitter](https://twitter.com/sharpmelk), [GitHub](https://github.com/sharpmel/STROBECourse), or [e-mail](mailto:melissaksharp@gmail.com).  
- 
Bland, J. M., & Altman, D. G. (1997). Statistics notes: Cronbach’s alpha. BMJ, 314(7080), 572. https://doi.org/10.1136/bmj.314.7080.572 [@bland1997]  
- BMJ Publishing Group. (2018). How to estimate the effect of treatment duration on survival outcomes using observational data. BMJ, 360, k182. https://doi.org/10.1136/bmj.k182 [@BMJ2018]  
- 
Cronbach, L. J. (1951). Coefficient alpha and the internal structure of tests. Psychometrika, 16(3), 297–334. https://doi.org/10.1007/BF02310555 [@cronbach1951]  
- Gamble, C., Krishan, A., Stocken, D., Lewis, S., Juszczak, E., Dore, C., Williamson, P. R., Altman, D. G., Montgomery, A., Lim, P., Berlin, J., Senn, S., Day, S., Barbachano, Y., & Loder, E. (2017). Guidelines for the Content of Statistical Analysis Plans in Clinical Trials. JAMA, 318(23), 2337–2343. https://doi.org/10.1001/jama.2017.18556 [@gamble2017]  
- Greenland, S., Senn, S. J., Rothman, K. J., Carlin, J. B., Poole, C., Goodman, S. N., & Altman, D. G. (2016). Statistical tests, P values, confidence intervals, and power: A guide to misinterpretations. European Journal of Epidemiology, 31, 337–350.   https://doi.org/10.1007/s10654-016-0149-3 [@greenland2016]  
- Ibrahim, J. G., & Molenberghs, G. (2009). Missing data methods in longitudinal studies: a review. Test (Madrid, Spain), 18(1), 1–43. doi:10.1007/s11749-009-0138-x [@ibrahim2009]  
- 
Knol, M. J., Egger, M., Scott, P., Geerlings, M. I., & Vandenbroucke, J. P. (2009). When One Depends on the Other: Reporting of Interaction in Case-Control and Cohort Studies. Epidemiology, 20(2), 161. https://doi.org/10.1097/EDE.0b013e31818f6651 [@knol2009]  
- Lesko, C. R., Edwards, J. K., Cole, S. R., Moore, R. D., & Lau, B. (2018). When to Censor? American Journal of Epidemiology, 187(3), 623–632. https://doi.org/10.1093/aje/kwx281 [@lesko2018]  
- Mansournia, M. A., Etminan, M., Danaei, G., Kaufman, J. S., & Collins, G. (2017). Handling time varying confounding in observational research. BMJ, 359. https://doi.org/10.1136/bmj.j4587 [@mansournia2017]  
- Smeden, M. van, Moons, K. G., Groot, J. A. de, Collins, G. S., Altman, D. G., Eijkemans, M. J., & Reitsma, J. B. (2018). Sample size for binary logistic prediction models: Beyond events per variable criteria. Statistical Methods in Medical Research, 0962280218784726. https://doi.org/10.1177/0962280218784726 [@vansmeden2018]  
- Smeden, M. van, Lash, T. L., & Groenwold, R. H. H. (2019). Reflection on modern methods: Five myths about measurement error in epidemiological research. International Journal of Epidemiology, dyz251. https://doi.org/10.1093/ije/dyz251 [@vansmeden2019]  

