---
title: "17-OtherAnalyses"
author: "Melissa K Sharp"
delete_merged_file: true
site: bookdown::bookdown_site
output:
  bookdown::gitbook:
    number_sections: false 
    includes: 
      after_body: disqus.html
bibliography: bibliography.bib
csl: apa-annotated.csl
link-citations: yes
edit: https://github.com/sharpmel/STROBECourse
---

# Results: Other Analyses (17)
> The items from STROBE state that you should report: 
-	Report other analyses done e.g., analyses of subgroups and interactions, and sensitivity analyses

-	EULAR(A) Consider performing analyses to explore possible effect modiﬁcation
-	EULAR(B) Consider performing sensitivity analyses for differing deﬁnitions of exposure (item 12A) and outcome or different statistical models, if applicable
-	strega17 (c) If detailed results are available elsewhere, state how they can be accessed.
-	nut-17. Report any sensitivity analysis (e.g., exclusion of misreporters or outliers) and data imputation, if applicable.
-	vet17 Report other analyses done, such as sensitivity/ robustness analysis and analysis of subgroups

## Explanation and Elaboration [@vandenbroucke2007]
> In addition to the main analysis other analyses are often done in observational studies. They may address specific subgroups, the potential interaction between risk factors, the calculation of attributable risks, or use alternative definitions of study variables in sensitivity analyses.
</br>
There is debate about the dangers associated with subgroup analyses, and multiplicity of analyses in general. 4,104 In our opinion, there is too great a tendency to look for evidence of subgroup-specific associations, or effect-measure modification, when overall results appear to suggest little or no effect. On the other hand, there is value in exploring 
whether an overall association appears consistent across several, preferably pre-specified subgroups especially when a 
study is large enough to have sufficient data in each subgroup. A second area of debate is about interesting subgroups that arose during the data analysis. They might be important findings, but might also arise by chance. Some argue that it is neither possible nor necessary to inform the reader about all subgroup analyses done as future analyses of other data will tell to what extent the early exciting findings stand the test of time.9 We advise authors to report which analyses were planned, and which were not (see also items 4, 12b and 20). This will allow readers to judge the implications of multiplicity, taking into account the study’s position on the continuum from discovery to verification or refutation.
</br>
A third area of debate is how joint effects and interactions between risk factors should be evaluated: on additive or 
multiplicative scales, or should the scale be determined by the statistical model that fits best (see also item 12b and box 8)? A sensible approach is to report the separate effect of each exposure as well as the joint effect – if possible in a table, as in the first example above,183 or in the study by Martinelli et al.185 Such a table gives the reader sufficient information to evaluate additive as well as multiplicative interaction (how these calculations are done is shown in box 8). </br>
Confidence intervals for separate and joint effects may help the reader to judge the strength of the data. In addition, confidence intervals around measures of interaction, such as the Relative Excess Risk from Interaction (RERI) relate to tests of interaction or homogeneity tests. One recurrent problem is that authors use comparisons of P values across subgroups, which lead to erroneous claims about an effect modifier. For instance, a statistically significant association in one category (eg, men), but not in the other (eg, women) does not in itself provide evidence of effect modification. Similarly, the confidence intervals for each point estimate are sometimes inappropriately used to infer that there is no interaction whintervals overlap. A more valid inference is achieved by directly evaluating whether the magnitude of an association differs across subgroups.
</br>
Sensitivity analyses are helpful to investigate the influence of choices made in the statistical analysis, or to investigate the robustness of the findings to missing data or possible biases (see also item 12b). Judgement is needed regarding the level of reporting of such analyses. If many sensitivity analyses were performed, it may be impractical to present detailed findings for them all. It may sometimes be sufficient to report that sensitivity analyses were carried out and that they were consistent with the main results presented. Detailed presentation is more appropriate if the issue investigated is of major concern, or if effect estimates vary considerably.59,186
</br>
Pocock and colleagues found that 43 out of 73 articles reporting observational studies contained subgroup analyses. The majority claimed differences across groups but only eight articles reported a formal evaluation of interaction (see item 12b).4

**Examples of Good Reporting**</br>
</br></br>

<h2> Field-specific guidance:</h2>
**Anti-microbial stewardship programs [@tacconelli2016]**
- Report subgroup analysis by type of patients and type of microorganism, if applicable </br>

**Genetic association studies [@little2009]**
- If numerous genetic exposures (genetic variants) were examined, summarize results from all analyses undertaken. </br>

**Response-driven sampling [@white2015]**
- Report other analyses done—for example, analyses of subgroups and interactions, sensitivity analyses, different RDS estimators and definitions of personal network size </br>